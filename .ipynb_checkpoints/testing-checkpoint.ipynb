{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import M\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from yolo import Yolo\n",
    "from loss import Loss\n",
    "from utils import  AverageMeter,class_accuracy\n",
    "from dataset import get_data\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Yolo(3,6//2,2)\n",
    "model.load_state_dict(torch.load('model_1111.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 13, 13, 7]), torch.Size([1, 3, 26, 26, 7]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,416,416)\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "pred[0].shape, pred[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBox(nn.Module):\n",
    "    def __init__(self, scaled_anchors, num_classes, img_size):\n",
    "        super(DecodeBox, self).__init__()\n",
    "        self.scaled_anchors = scaled_anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.bbox_attrs = 5 + num_classes\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        input_height = input.size(2)\n",
    "        input_width = input.size(3)\n",
    "        stride_h = self.img_size[1] / input_height\n",
    "        stride_w = self.img_size[0] / input_width\n",
    "        scaled_anchors = self.scaled_anchors\n",
    "\n",
    "        prediction = input.view(batch_size, self.num_anchors,\n",
    "                                self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = torch.sigmoid(prediction[..., 0])\n",
    "        y = torch.sigmoid(prediction[..., 1])\n",
    "        w = prediction[..., 2]\n",
    "        h = prediction[..., 3]\n",
    "        conf = torch.sigmoid(prediction[..., 4])\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(\n",
    "            batch_size * self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)\n",
    "        grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(\n",
    "            batch_size * self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)\n",
    "        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))\n",
    "        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))\n",
    "        anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)\n",
    "        anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + grid_x\n",
    "        pred_boxes[..., 1] = y.data + grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n",
    "        scale = torch.Tensor([stride_w, stride_h] * 2).type(FloatTensor)\n",
    "        output = torch.cat((pred_boxes.view(batch_size, -1, 4) * scale,\n",
    "                            conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1)\n",
    "        return output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [13,26]\n",
    "ANCHORS =  [[(0.275 ,   0.320312), (0.068   , 0.113281), (0.017  ,  0.03   )], \n",
    "           [(0.03  ,   0.056   ), (0.01  ,   0.018   ), (0.006 ,   0.01    )]]\n",
    "\n",
    "scaled_anchors = (\n",
    "        torch.tensor(ANCHORS)\n",
    "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
